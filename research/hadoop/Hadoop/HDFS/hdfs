=== HADOOP HDFS FUSE

HDFS Architecture Guide 

http://hadoop.apache.org/common/docs/stable/hdfs_design.html

- Simple Coherency Model
HDFS applications need a write-once-read-many access model for files. A file once created, written, and closed need not be changed. *
This assumption simplifies data coherency issues and enables high throughput data access. A MapReduce application or a web crawler application fits perfectly with this model. 
There is a plan to support appending-writes to files in the future. *

- The File System Namespace 
HDFS supports a traditional hierarchical file organization.
A user or an application can create directories and store files inside these directories. The file system namespace hierarchy is similar to most other existing file systems; 
one can create and remove files, move a file from one directory to another, or rename a file. *
HDFS does not yet implement user quotas. HDFS does not support hard links or soft links. However, the HDFS architecture does not preclude implementing these features. 

=== environment 
hadoop-0.20.2+737


=== FUSE-HDFS
http://fuse.sourceforge.net/                   (install fuse)
http://wiki.apache.org/hadoop/MountableHDFS

Fuse-DFS
Supports reads, writes, and directory operations (e.g., cp, ls, more, cat, find, less, rm, mkdir, mv, rmdir). Things like touch, chmod, chown, and permissions are in the works. Fuse-dfs currently shows all files as owned by nobody. 

http://blog.sina.com.cn/s/blog_5cf546320100is1d.html
http://savagegarden.iteye.com/blog/1170752


2012.05.14

--- BUILDING
src/contrib/fuse-dfs

$ ant compile-c++-libhdfs -Dislibhdfs=1  (注：-Dlibhdfs=1 可能导致libhdfs未编译)
$ ant package

---> error
/home/zhongxia/workspace/hadoop-0.20.203.0/build.xml:1220: 'java5.home' is not defined.  Forrest requires Java 5.  Please pass -Djava5.home=<base of Java 5 distribution> to Ant on the command-line.
----
1. 在build.xml中注释掉Documentation相关的target (含forrest.check), 在target(<target name="package")中去掉对doc相关target的依赖。
2. 注释掉 target: test-patch, hudson-test-patch

----------------------------------------------
$ cd build
$ mkdir libhdfs
$ cp ../c++/Linux-amd64-64/lib/* libhdfs/
$ cd ../
---------------------------------------------------
$ ant compile-contrib -Dlibhdfs=1 -Dfusedfs=1

---> error
gcc  -Wall -O3 -L/home/zhongxia/Software/hadoop-0.20.2+737/build/c++/Linux-amd64-64/lib -lhdfs -L/lib -lfuse -L/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/amd64/server -ljvm -o fuse_dfs fuse_dfs.o fuse_options.o fuse_trash.o fuse_stat_struct.o fuse_users.o fuse_init.o fuse_connect.o fuse_impls_access.o fuse_impls_chmod.o fuse_impls_chown.o fuse_impls_create.o fuse_impls_flush.o fuse_impls_getattr.o fuse_impls_mkdir.o fuse_impls_mknod.o fuse_impls_open.o fuse_impls_read.o fuse_impls_release.o fuse_impls_readdir.o fuse_impls_rename.o fuse_impls_rmdir.o fuse_impls_statfs.o fuse_impls_symlink.o fuse_impls_truncate.o fuse_impls_utimens.o fuse_impls_unlink.o fuse_impls_write.o 
[exec] fuse_dfs.o: In function `is_protected':
[exec] /home/zhongxia/Software/hadoop-0.20.2+737/src/contrib/fuse-dfs/src/fuse_dfs.c:(.text+0xa): undefined reference to `fuse_get_context'
       ...
       ...
----
-- http://wiki.apache.org/hadoop/BuildFuseDfs023
by moving all the -L and -l part at the end, then:
$ cd src/contrib/fuse-dfs/src/
gcc  -Wall -O3 -o fuse_dfs fuse_dfs.o fuse_options.o fuse_trash.o fuse_stat_struct.o fuse_users.o fuse_init.o fuse_connect.o fuse_impls_access.o fuse_impls_chmod.o fuse_impls_chown.o fuse_impls_create.o fuse_impls_flush.o fuse_impls_getattr.o fuse_impls_mkdir.o fuse_impls_mknod.o fuse_impls_open.o fuse_impls_read.o fuse_impls_release.o fuse_impls_readdir.o fuse_impls_rename.o fuse_impls_rmdir.o fuse_impls_statfs.o fuse_impls_symlink.o fuse_impls_truncate.o fuse_impls_utimens.o fuse_impls_unlink.o fuse_impls_write.o -L/home/zhongxia/Software/hadoop-0.20.2+737/build/c++/Linux-amd64-64/lib -lhdfs -L/lib -lfuse -L/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/amd64/server -ljvm

---> error
fuse_connect.c:40: error: too many arguments to function ‘hdfsConnectAsUser’
---- see, https://issues.apache.org/jira/browse/HDFS-1267
src/contrib/fuse-dfs/src/fuse_connect.c
-  hdfsFS fs = hdfsConnectAsUser(hostname, port, user, (const char **)groups, numgroups);
+  hdfsFS fs = hdfsConnectAsUser(hostname, port, user);

--- CONFIGURING & MOUNT

$ cd build/contrib/fuse-dfs/
$ sudo mkdir /mnt/hdfs 
$ mkdir /tmp/hdfs
$ ./fuse_dfs_wrapper.sh dfs://172.16.0.161:9000 /tmp/hdfs

---> error
./fuse_dfs: error while loading shared libraries: libhdfs.so.0: cannot open shared object file: No such file or directory
----
在 fuse_dfs_wrapper.sh 设置正确的路径，
HADOOP_HOME=...
LD_LIBRARY_PATH=...:$HADOOP_HOME/build/libhdfs:$LD_LIBRARY_PATH

---> error
fusermount: failed to open /etc/fuse.conf: Permission denied
fusermount: option allow_other only allowed if 'user_allow_other' is set in /etc/fuse.conf
----
$ sudo chmod a+r /etc/fuse.conf
$ sudo vi /etc/fuse.conf
#user_allow_other 去掉注释，允许非root用户进行mount操作。

---> error
./fuse_dfs: error while loading shared libraries: libjvm.so: cannot open shared object file: No such file or directory
---
check
$ ldd fuse_dfs
libjvm.so => not found

$ sudo ldconfig -p | grep libjvm ??

fuse_dfs_wrapper.sh
--
LD_LIBRARY_PATH=/usr/lib/jvm/java-6-openjdk/jre/lib/amd64/server:...
--
 or
sudo ln -s /usr/lib/jvm/java-6-openjdk/jre/lib/amd64/server/libjvm.so /lib/libjvm.so

---> error
fuse-dfs didn't recognize /tmp/hdfs,-2
---


==  其他

修改cluster配置hdfs-site.xml, 取消权限验证，方便从集群外的客户端机器访问。 ？？？
<configuration>
        <property>
                <name>dfs.permissions</name>
                <value>false</value>
        </property> 
</configuration>

--->
/bin/mount: unrecognized option '--no-canonicalize'







